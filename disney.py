import re
import asyncio
from typing import Any
from bs4 import BeautifulSoup
import requests
from playwright.async_api import async_playwright

def extract_price(html: str) -> list[dict[str, Any]]:
    soup = BeautifulSoup(html, 'html.parser')
    all_tables = soup.find_all('table') # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼
    all_plans = []

    if not all_tables:
        return []

    for table in all_tables: # éå†æ¯ä¸ªè¡¨æ ¼
        # åœ¨è¿™é‡Œå¯ä»¥æ·»åŠ ä¸€äº›åˆ¤æ–­ï¼Œæ£€æŸ¥è¿™ä¸ª table æ˜¯å¦æ˜¯éœ€è¦çš„ä»·æ ¼è¡¨
        # ä¾‹å¦‚ï¼š if "Price" not in str(table.find('tr')): continue

        plans_in_table = []
        try: # å¢åŠ é”™è¯¯å¤„ç†ï¼Œé˜²æ­¢æŸä¸ªè¡¨æ ¼ç»“æ„ä¸ç¬¦åˆé¢„æœŸå¯¼è‡´æ•´ä¸ªå‡½æ•°å¤±è´¥
            for row in table.find_all('tr')[1:]: # å¯¹å½“å‰è¡¨æ ¼åº”ç”¨è§£æé€»è¾‘
                cols = row.find_all('td')
                # å¯èƒ½éœ€è¦æ ¹æ®ä¸åŒè¡¨æ ¼è°ƒæ•´è¿™é‡Œçš„åˆ—æ•°åˆ¤æ–­å’Œç´¢å¼•
                if len(cols) >= 3: # æˆ–è€…æ›´çµæ´»çš„åˆ¤æ–­
                    plan = cols[0].get_text(strip=True)
                    # ä»·æ ¼åˆ—çš„ç´¢å¼•ä¹Ÿå¯èƒ½å˜åŒ–
                    price = ' '.join(cols[2].get_text(separator=' ', strip=True).split())
                    # é¿å…æ·»åŠ ç©ºçš„ plan æˆ– price
                    if plan and price:
                         plans_in_table.append({'plan': plan, 'price': price})
                # å¯ä»¥è€ƒè™‘æ·»åŠ  elif len(cols) == X: æ¥å¤„ç†å…¶ä»–ç»“æ„çš„è¡¨æ ¼
        except Exception as e:
             print(f"è§£ææŸä¸ªè¡¨æ ¼æ—¶å‡ºé”™: {e}") # æ‰“å°é”™è¯¯ä¿¡æ¯ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªè¡¨æ ¼

        all_plans.extend(plans_in_table) # å°†å½“å‰è¡¨æ ¼è§£æå‡ºçš„å¥—é¤åŠ å…¥æ€»åˆ—è¡¨

    return all_plans


def get_request_json(article_id: str, selected_meta: str, country: str) -> dict:
    return {
        "namespace": "",
        "classname": "@udd/01p5f00000ebl3g",
        "method": "loadArticle",
        "isContinuation": False,
        "params": {
            "articleId": article_id,
            "brand": "Disney",
            "country": country,
            "selectedMeta": selected_meta,
            "isPreview": False
        },
        "cacheable": False
    }


def get_price_json(article_id: str, selected_meta: str, country: str, localeCode: str) -> dict:
    url = f'https://help.disneyplus.com/{localeCode}/webruntime/api/apex/execute'
    resp = requests.post(url, json=get_request_json(article_id, selected_meta, country))
    resp.raise_for_status()
    return resp.json()


def get_country_language_localization() -> dict[str, Any]:
    # é»˜è®¤ä½¿ç”¨å¾·è¯­æ¥å£è·å–ï¼Œä½†åç»­ä¼šä¼˜å…ˆé€‰å– en-* locale
    url = (
        'https://help.disneyplus.com/de/webruntime/api/apex/execute'
        '?cacheable=true&classname=%40udd%2F01p5f00000e1rTi'
        '&isContinuation=false&method=getCountryLanguageLocalization'
        '&namespace=&params=%7B%22brand%22%3A%22Disney%22%2C%22selectedLanguage%22%3A%22de%22%7D'
        '&language=de&asGuest=true&htmlEncode=false'
    )
    resp = requests.get(url)
    resp.raise_for_status()
    return resp.json()['returnValue']


async def fetch_record_id(browser, locale_code: str) -> str:
    page = await browser.new_page()
    try:
        await page.goto(
            f'https://help.disneyplus.com/{locale_code}/article/disneyplus-price',
            wait_until='domcontentloaded'
        )
        html = await page.content()
        matches = re.findall(r'\{"recordId":"(.*?)"\}', html)
        if not matches:
            raise ValueError(f"No recordId found for locale {locale_code}")
        return matches[0]
    finally:
        await page.close()


async def main():
    loc_map = get_country_language_localization()
    results: dict[str, Any] = {}

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)

        for country_code, info in loc_map.items():
            # æ‰€æœ‰å¯ç”¨è¯­è¨€é€‰é¡¹
            lan_entries = info.get('lanInfo', [])
            # ä¼˜å…ˆé€‰æ‹©ä»¥ en- å¼€å¤´çš„ English locale
            en_entries = [l for l in lan_entries if l.get('localeCode', '').startswith('en-')]
            if en_entries:
                lan = en_entries[0]
            else:
                lan = lan_entries[0]

            locale_code = lan['localeCode']
            master_label = lan['masterLabel']
            try:
                # è·å– recordId
                record_id = await fetch_record_id(browser, locale_code)
                # è¯·æ±‚æ–‡ç«  JSON
                price_json = get_price_json(record_id, master_label, country_code, locale_code)

                # æå– HTML ç‰‡æ®µå’Œ LastPublishedDate
                html_fragment = price_json['returnValue']['HowTo_Details__c']
                last_published_date = price_json['returnValue'].get('LastPublishedDate')

                # è§£æå¥—é¤ä¿¡æ¯
                plans = extract_price(html_fragment)
                # å°† LastPublishedDate åŠ å…¥æ¯ä¸ªå¥—é¤å­—å…¸ä¸­
                for plan in plans:
                    plan['last_published_date'] = last_published_date

                results[country_code] = plans
                print(f"[{country_code}] ä½¿ç”¨ {locale_code} æŠ“å–åˆ° {len(plans)} ä¸ªå¥—é¤ï¼Œå‘å¸ƒæ—¥æœŸ: {last_published_date}")
            except Exception as e:
                print(f"[{country_code}] å¤±è´¥ï¼š{e}")

        await browser.close()

    return results


def extract_year_from_timestamp(timestamp: str) -> str:
    """ä»æ—¶é—´æˆ³ä¸­æå–å¹´ä»½"""
    try:
        # æ—¶é—´æˆ³æ ¼å¼: YYYYMMDD_HHMMSS
        if len(timestamp) >= 4:
            return timestamp[:4]
        else:
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›å½“å‰å¹´ä»½
            return time.strftime('%Y')
    except:
        return time.strftime('%Y')

def create_archive_directory_structure(archive_dir: str, timestamp: str) -> str:
    """æ ¹æ®æ—¶é—´æˆ³åˆ›å»ºæŒ‰å¹´ä»½ç»„ç»‡çš„å½’æ¡£ç›®å½•ç»“æ„"""
    year = extract_year_from_timestamp(timestamp)
    year_dir = os.path.join(archive_dir, year)
    if not os.path.exists(year_dir):
        os.makedirs(year_dir)
        print(f"ğŸ“ åˆ›å»ºå¹´ä»½ç›®å½•: {year_dir}")
    return year_dir

def migrate_existing_archive_files(archive_dir: str):
    """å°†ç°æœ‰çš„å½’æ¡£æ–‡ä»¶è¿ç§»åˆ°æŒ‰å¹´ä»½ç»„ç»‡çš„ç›®å½•ç»“æ„ä¸­"""
    if not os.path.exists(archive_dir):
        return
    
    migrated_count = 0
    
    # æŸ¥æ‰¾æ ¹ç›®å½•ä¸‹çš„å½’æ¡£æ–‡ä»¶
    for filename in os.listdir(archive_dir):
        if filename.startswith('disneyplus_prices_') and filename.endswith('.json'):
            file_path = os.path.join(archive_dir, filename)
            
            # ç¡®ä¿æ˜¯æ–‡ä»¶è€Œä¸æ˜¯ç›®å½•
            if os.path.isfile(file_path):
                # ä»æ–‡ä»¶åæå–æ—¶é—´æˆ³
                try:
                    # æ–‡ä»¶åæ ¼å¼: disneyplus_prices_YYYYMMDD_HHMMSS.json
                    timestamp_part = filename.replace('disneyplus_prices_', '').replace('.json', '')
                    year = extract_year_from_timestamp(timestamp_part)
                    
                    # åˆ›å»ºå¹´ä»½ç›®å½•
                    year_dir = create_archive_directory_structure(archive_dir, timestamp_part)
                    
                    # ç§»åŠ¨æ–‡ä»¶
                    new_path = os.path.join(year_dir, filename)
                    if not os.path.exists(new_path):  # é¿å…é‡å¤ç§»åŠ¨
                        shutil.move(file_path, new_path)
                        print(f"ğŸ“¦ è¿ç§»æ–‡ä»¶: {filename} â†’ {year}/")
                        migrated_count += 1
                except Exception as e:
                    print(f"âš ï¸  è¿ç§»æ–‡ä»¶å¤±è´¥ {filename}: {e}")
    
    if migrated_count > 0:
        print(f"âœ… æˆåŠŸè¿ç§» {migrated_count} ä¸ªå½’æ¡£æ–‡ä»¶åˆ°å¹´ä»½ç›®å½•")
    else:
        print("ğŸ“‚ æ²¡æœ‰éœ€è¦è¿ç§»çš„å½’æ¡£æ–‡ä»¶")

def get_archive_statistics(archive_dir: str) -> dict:
    """è·å–å½’æ¡£æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯"""
    if not os.path.exists(archive_dir):
        return {"total_files": 0, "years": {}}
    
    stats = {"total_files": 0, "years": {}}
    
    # éå†æ‰€æœ‰å¹´ä»½ç›®å½•
    for item in os.listdir(archive_dir):
        item_path = os.path.join(archive_dir, item)
        if os.path.isdir(item_path) and item.isdigit() and len(item) == 4:
            year = item
            year_files = []
            
            # ç»Ÿè®¡è¯¥å¹´ä»½çš„æ–‡ä»¶
            for filename in os.listdir(item_path):
                if filename.startswith('disneyplus_prices_') and filename.endswith('.json'):
                    filepath = os.path.join(item_path, filename)
                    mtime = os.path.getmtime(filepath)
                    year_files.append((filepath, mtime, filename))
            
            # æŒ‰æ—¶é—´æ’åº
            year_files.sort(key=lambda x: x[1], reverse=True)
            stats["years"][year] = {
                "count": len(year_files),
                "files": year_files
            }
            stats["total_files"] += len(year_files)
    
    return stats


import json
import time
import os
import shutil

def extract_year_from_timestamp(timestamp: str) -> str:
    """ä»æ—¶é—´æˆ³ä¸­æå–å¹´ä»½"""
    try:
        # æ—¶é—´æˆ³æ ¼å¼: YYYYMMDD_HHMMSS
        if len(timestamp) >= 4:
            return timestamp[:4]
        else:
            # å¦‚æœè§£æå¤±è´¥ï¼Œè¿”å›å½“å‰å¹´ä»½
            return time.strftime('%Y')
    except:
        return time.strftime('%Y')

def create_archive_directory_structure(archive_dir: str, timestamp: str) -> str:
    """æ ¹æ®æ—¶é—´æˆ³åˆ›å»ºæŒ‰å¹´ä»½ç»„ç»‡çš„å½’æ¡£ç›®å½•ç»“æ„"""
    year = extract_year_from_timestamp(timestamp)
    year_dir = os.path.join(archive_dir, year)
    if not os.path.exists(year_dir):
        os.makedirs(year_dir)
        print(f"ğŸ“ åˆ›å»ºå¹´ä»½ç›®å½•: {year_dir}")
    return year_dir

def migrate_existing_archive_files(archive_dir: str):
    """å°†ç°æœ‰çš„å½’æ¡£æ–‡ä»¶è¿ç§»åˆ°æŒ‰å¹´ä»½ç»„ç»‡çš„ç›®å½•ç»“æ„ä¸­"""
    if not os.path.exists(archive_dir):
        return
    
    migrated_count = 0
    
    # æŸ¥æ‰¾æ ¹ç›®å½•ä¸‹çš„å½’æ¡£æ–‡ä»¶
    for filename in os.listdir(archive_dir):
        if filename.startswith('disneyplus_prices_') and filename.endswith('.json'):
            file_path = os.path.join(archive_dir, filename)
            
            # ç¡®ä¿æ˜¯æ–‡ä»¶è€Œä¸æ˜¯ç›®å½•
            if os.path.isfile(file_path):
                # ä»æ–‡ä»¶åæå–æ—¶é—´æˆ³
                try:
                    # æ–‡ä»¶åæ ¼å¼: disneyplus_prices_YYYYMMDD_HHMMSS.json
                    timestamp_part = filename.replace('disneyplus_prices_', '').replace('.json', '')
                    year = extract_year_from_timestamp(timestamp_part)
                    
                    # åˆ›å»ºå¹´ä»½ç›®å½•
                    year_dir = create_archive_directory_structure(archive_dir, timestamp_part)
                    
                    # ç§»åŠ¨æ–‡ä»¶
                    new_path = os.path.join(year_dir, filename)
                    if not os.path.exists(new_path):  # é¿å…é‡å¤ç§»åŠ¨
                        shutil.move(file_path, new_path)
                        print(f"ğŸ“¦ è¿ç§»æ–‡ä»¶: {filename} â†’ {year}/")
                        migrated_count += 1
                except Exception as e:
                    print(f"âš ï¸  è¿ç§»æ–‡ä»¶å¤±è´¥ {filename}: {e}")
    
    if migrated_count > 0:
        print(f"âœ… æˆåŠŸè¿ç§» {migrated_count} ä¸ªå½’æ¡£æ–‡ä»¶åˆ°å¹´ä»½ç›®å½•")
    else:
        print("ğŸ“‚ æ²¡æœ‰éœ€è¦è¿ç§»çš„å½’æ¡£æ–‡ä»¶")

def get_archive_statistics(archive_dir: str) -> dict:
    """è·å–å½’æ¡£æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯"""
    if not os.path.exists(archive_dir):
        return {"total_files": 0, "years": {}}
    
    stats = {"total_files": 0, "years": {}}
    
    # éå†æ‰€æœ‰å¹´ä»½ç›®å½•
    for item in os.listdir(archive_dir):
        item_path = os.path.join(archive_dir, item)
        if os.path.isdir(item_path) and item.isdigit() and len(item) == 4:
            year = item
            year_files = []
            
            # ç»Ÿè®¡è¯¥å¹´ä»½çš„æ–‡ä»¶
            for filename in os.listdir(item_path):
                if filename.startswith('disneyplus_prices_') and filename.endswith('.json'):
                    filepath = os.path.join(item_path, filename)
                    mtime = os.path.getmtime(filepath)
                    year_files.append((filepath, mtime, filename))
            
            # æŒ‰æ—¶é—´æ’åº
            year_files.sort(key=lambda x: x[1], reverse=True)
            stats["years"][year] = {
                "count": len(year_files),
                "files": year_files
            }
            stats["total_files"] += len(year_files)
    
    return stats

if __name__ == '__main__':
    all_prices = asyncio.run(main())
    
    timestamp = time.strftime('%Y%m%d_%H%M%S')
    output_file = f'disneyplus_prices_{timestamp}.json'
    output_file_latest = 'disneyplus_prices.json'
    
    # ç¡®ä¿å½’æ¡£ç›®å½•ç»“æ„å­˜åœ¨
    archive_dir = 'archive'
    if not os.path.exists(archive_dir):
        os.makedirs(archive_dir)
        
    # æ£€æŸ¥å¹¶è¿ç§»ç°æœ‰çš„å½’æ¡£æ–‡ä»¶åˆ°å¹´ä»½ç›®å½•
    migrate_existing_archive_files(archive_dir)
    
    # æ ¹æ®æ—¶é—´æˆ³åˆ›å»ºå¹´ä»½å­ç›®å½•
    year_archive_dir = create_archive_directory_structure(archive_dir, timestamp)
    
    # ä¿å­˜å¸¦æ—¶é—´æˆ³çš„ç‰ˆæœ¬åˆ°å¯¹åº”å¹´ä»½å½’æ¡£ç›®å½•
    archive_file = os.path.join(year_archive_dir, output_file)
    with open(archive_file, 'w', encoding='utf-8') as f:
        json.dump(all_prices, f, ensure_ascii=False, indent=2)
        
    # ä¿å­˜æœ€æ–°ç‰ˆæœ¬ï¼ˆä¾›è½¬æ¢å™¨ä½¿ç”¨ï¼‰
    with open(output_file_latest, 'w', encoding='utf-8') as f:
        json.dump(all_prices, f, ensure_ascii=False, indent=2)
        
    print(f"å·²å†™å…¥ {output_file_latest} å’Œ {archive_file}")
